---
title: "PS8 - Scatterplots and Correlation"
format: pdf
jupyter: python3
---

# Introduction

Please watch [Correlation Does Not Equal Causation](https://www.youtube.com/watch?v=GtV-VYdNt_g&list=PL8dPuuaLjXtNM_Y-bUAhblSAdWRnmBUcr&index=9&t=1s) before starting the problem set.

# Setup

For this problem set we are going to load several new packages: `scikit-learn`, `statsmodels`, and `pydataset`.  We will use `scikit-learn`, and `statsmodels` for doing statistical analysis. Both of these packages contain much of the same functionality, but their focus is a little different. The `pydataset` contains datasets we will practice doing statistics with.

This problem set uses the Diabetes dataset from `scikit-learn` as an example. 

```{python}
import pandas as pd  
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
import numpy.random as rand #load random number generation from numpy


#set options to always display all columns in DataFrame
pd.set_option('display.max_columns', None)

# load diabetes data
diabetes_data = load_diabetes(as_frame=True)

#extract data frame
diabetes = diabetes_data.frame

```


# What Question Are You Asking?

Whenever you sit down to do statistics, it is always a good idea to keep in mind what question you are trying to answer. In general, there are two types of questions statisticians can ask: questions of prediction and questions of inference.

## Questions of prediction
Questions of prediction are questions that ask us to predict the outcome of a situation we do not know. For example, we may want to know how a student will do on a final, based on their grades during the rest of the class. We may want to predict which basketball team match-ups will be most competitive, so we don't buy tickets to a boring game. A very common prediction question is what will the weather be like tomorrow, or a week from tomorrow. For the diabetes dataset, we are interested in predicting how quickly a person's diabetes will progress once diagnosed. This is particularly of interest because uncontrolled diabetes can have very severe health outcomes. 

## Questions of inference
Questions of inference are questions about how the world works. For example, we may want to know what effect an increase in atmospheric CO~2~ has summer temperatures in the Central Valley, or the severity of fire season. This won't tell us anything about what to wear next week, but it will inform whether or not we decide to move to a different state. For the diabetes dataset, we want to know what are the most useful risk factors to look for when assessing whether a person is likely to develop type II diabetes.

## Spurious Inference
Flying Spaghetti Monster

# Summarizing the Dataset

Before we do anything too fancy, it can be helpful to get a sense of what type of data we have. Are the variables numerical or categorical? Are the numeric variables integers are or decimal numbers? What range of values do the variables take? These questions are because the they will affect type of analysis we can do. There are many more types of analysis we can do on numeric data than on categorical data, but we don't always have that option. 

```{python}

print(diabetes.describe())

```

Looking at the data, we can see something a little bit peculiar about it. All of the variables except for `target` have the same mean and standard deviation. This is definitely not normal. Also the variables are not very well labeled. In particular, 6 of the variables are labelled `s1`-`s6`. Not very informative. Thankfully we can look at the documentation to get some explanations.

```{python}

# get documenation on diabetes data
print(diabetes_data.DESCR)

```

The documentation gives us a number of pieces of information. First, the variable `target` is a measure of disease severity one year after diagnosis. So we will probably focus on predicting that variable. Also, we have descriptions of all of the variables, so we can label them in a way that is more helpful. Finally, there is a note stating that all the variables except `target` have been "mean centered and scaled by the standard deviation". What that means is that the researchers subtracted the mean and divided by the standard deviation for each data point in each of the 10 'predictor' variables. Researchers do this to make it easier to compare effect sizes between variables.

```{python}
namedict = {'s1': 'totchol', 's2':'ldl', 's3':'hdl', 's4':'tch', 
's5':'ltg', 's6':'glucose'}

diabetes=diabetes.rename(columns=namedict)

print(diabetes.head())

```


# Visualizing The Dataset 

Whenever we have a new dataset to analyze, a good second step is visualizing the variables in the dataset. For the Diabetes dataset, we are specifically interested in disease severity so we will be using `target`, a measure of diabetes disease progression.  Basically it tells us how bad a person's diabetes is. Higher numbers mean the diabetes is worse. Figure 1 shows every variable in the dataset plotted against our measure of disease progression in scatterplots. 

By looking at the scatterplots, we can get a good idea of which variables are likely to help us understand diabetes better, or at least better predict disease severity. Body Mass Index and Blood Triglycerides are positively correlated with more severe diabetes while High-Density Lipoproteins (Good Cholesteral) is possibly negatively correlated with severe diabetes. For the other variables, the data is too 'noisy' or spread out to tell from a visual inspection.
```{python}

x_names = ['Age', 'Sex', 'BMI', 'Blood Pressure', 'Total Cholestral', 
"LDL ('Bad Cholesteral')", "HDL ('Good Cholesteral')", 'Total/HDL', 
'Log(Serum Triglycerides Level)', 'Blood Sugar Level']

ncol = 2
nrow = int(len(x_names)/ncol)

fig1, axs1 = plt.subplots(nrow, ncol, sharey=True, sharex=True,
figsize=(8,10))

for i, ax in enumerate(fig1.axes):
    ax.scatter(diabetes.iloc[:, i], diabetes.target)
    ax.set_xlabel(x_names[i])
    if i % 2 == 0:
        ax.set_ylabel('Disease Progression')

#Add Title
fig1.suptitle('Figure 1. Scatterplots of Diabetes Covariates')

#removing space between the plots horizontally 
plt.subplots_adjust(wspace=0, top=0.95, bottom=0.15)


plt.show()

```

# Quantifying the Data set

# Measuring Correlation
Correlation is the measure of how much one variable changes as another one changes. If an increase in one variable also leads in an increase in a second variable, these two variables are positively correlated. An example of this would be height and weight in humans. If an increase in one variable leads to a decrease in a second variable, these variables are negatively correlated. An example of negatively correlated variables are the hours of sleep you get and the likelihood of you getting in a traffic accident.

There are many ways of measuring correlation, but one of the most common is the [Pearson Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), which is represented with a lowercase $r$. Pearson's $r$ tells you how closely two variables follow a linear relationship (Figure 1). This means that there still could be a (non-linear) relationship between two variables even if $r$ is close to zero. That is why it is always important to plot your data!

![Pearson's Correlation Coefficent](data/plots/Correlation_examples2.png)

There are many functions in python you can use to calculate the Pearson correlation coefficient. The `pandas` package has a method called `corr` that calculates the correlation coefficents for all pairs of variables in a DataFrame. This is very useful and efficient, but you need to make sure you don't have any string variables in your DataFrame.

```{python }

diabetes.corr()

```


# Correlation $\neq$ Causation

Linear regression models assess the degree of correlation between two or more variables. However, just because two variables are correlated, does not mean one is necessarily caused by the other. This can happen in two primary ways: hidden causation and reverse causation. 

## Reverse Causation
One potential causation problem is if the causation is reversed: we use A to predict B, but in fact it is B that is causing A. This is the case with the [Waffle House Index](https://en.wikipedia.org/wiki/Waffle_House_Index). In the south, where Waffle Houses and hurricanes are both plentiful, a former member of the Federal Emergency Management Agency (FEMA) developed the Waffle House Index to assess potential damage and emergency response requirements early in a storm. This metric was based on how many Waffle Houses in the area were closed or on limited menus. Normally Waffle Houses are open 24/7, and they tend to remain open even in severe weather. So the more Waffle Houses were closed, the more severe FEMA would assume the damage would be. Now, obviously the Waffle House closures are not causing the storms to be bad, it is the other way around. However, because it is much easier to figure out whether a Waffle House is closed than assess property damage in a storm, we use the former to predict the latter.

## Confounding Factors
A confounding factor is a (often times unmeasured) variable that influences both the "cause" and the "effect". Consider a study trying to predict a child's reading ability. If you didn't understand anything about how the brain worked, you might decide to use a child's shoe size as a predictor of their reading skill. Now we all know that having a larger shoe size does not cause you to have a better ability to read. However, in children, both shoe size and reading ability increase with the confounding factor: age. 

## Spurious Correlation

Sometimes things are correlated by chance. For example, The Church of the Flying Spagetti Monster, a satirical religion, claims that since global pirate numbers have been falling since the early 1800s and average global temperatures has been rising since that time, climate change is a punishment for the lack of pirates. This is obviously not true, but it is a good example of how things can be correlated, even if they are entirely unrelated. A more famous, and harmful, example of this is a small study that came out in 1998 that claimed a link between the MMR vaccine and autism. It was later discovered that the author of the study ____, did not include all of his data. However, the damage had already been done. The hypothesis was originally put forward because people generally recieve the MMR vaccine around age 2, which is also the time people start exhibiting the first signs of autism.


# Questions
For this problem set please use the `mtcars` dataset from the `pydatasets` package to answer the following questions. 

1. Reading the Documentation: the `pydatasets` package does not have very good documetation for the the datasets it contains. Thankfully, `mtcars` is well documented elsewhere. 
   a. Find an alternate source of documetation for the `mtcars` dataset on the internet.
   b. Describe each variable in the `mtcars` dataset, including the units and data range if it is a numeric variable. 
2. Prediction vs. Inference
   a. What is a question of prediction we could ask about fuel efficiency using the `mtcars` data set.
   b. What is an inferential question we could ask about fuel efficiency using the `mtcars` data set. 
3. Visualization: create a figure with at least 3 scatterplot subplots, showing how fuel efficiency (mpg) changes.