---
title: "PS8 - Intro To Linear Regression"
format: pdf
jupyter: python3
---

# Introduction

Please watch [Correlation Does Not Equal Causation](https://www.youtube.com/watch?v=GtV-VYdNt_g&list=PL8dPuuaLjXtNM_Y-bUAhblSAdWRnmBUcr&index=9&t=1s) before starting the problem set.

# Setup

For this problem set we are going to load several new packages: `scikit-learn`, `statsmodels`.  We will use `scikit-learn`, and `statsmodels` for doing statistical analysis. Both of these packages contain much of the same functionality, but their focus is a little different.

This problem set uses the Diabetes dataset from `scikit-learn` as an example. 

```{python}
import pandas as pd  
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
import numpy.random as rand #load random number generation from numpy


#set options to always display all columns in DataFrame
pd.set_option('display.max_columns', None)

#pd.set_option('display.precision', 2)

pd.set_option('display.float_format', lambda x: '%.3f' % x)

# load diabetes data
diabetes_data = load_diabetes(as_frame=True)

#extract data frame
diabetes = diabetes_data.frame

```


# What Question Are You Asking?

Whenever you sit down to do statistics, it is always a good idea to keep in mind what question you are trying to answer. In general, there are two types of questions statisticians can ask: questions of prediction and questions of inference.

## Questions of prediction
Questions of prediction are questions that ask us to predict the outcome of a situation we do not know. For example, we may want to know how a student will do on a final, based on their grades during the rest of the class. We may want to predict which basketball team match-ups will be most competitive, so we don't buy tickets to a boring game. A very common prediction question is what will the weather be like tomorrow, or a week from tomorrow. For the diabetes dataset, we are interested in predicting how quickly a person's diabetes will progress once diagnosed. This is particularly of interest because uncontrolled diabetes can have very severe health outcomes. 

## Questions of inference
Questions of inference are questions about how the world works. For example, we may want to know what effect an increase in atmospheric CO~2~ has summer temperatures in the Central Valley, or the severity of fire season. This won't tell us anything about what to wear next week, but it will inform whether or not we decide to move to a different state. For the diabetes dataset, we want to know what are the most useful risk factors to look for when assessing whether a person is likely to develop type II diabetes.

# Summarizing the Dataset

Before we do anything too fancy, it can be helpful to get a sense of what type of data we have. Are the variables numerical or categorical? Are the numeric variables integers are or decimal numbers? What range of values do the variables take? These questions are because the they will affect type of analysis we can do. There are many more types of analysis we can do on numeric data than on categorical data, but we don't always have that option. 

```{python}

print(diabetes.describe())

```

Looking at the data, we can see something a little bit peculiar about it. All of the variables except for `target` have the same mean and standard deviation. This is definitely not normal. Also the variables are not very well labeled. In particular, 6 of the variables are labelled `s1`-`s6`. Not very informative. Thankfully we can look at the documentation to get some explanations.

```{python}

# get documenation on diabetes data
print(diabetes_data.DESCR)

```

The documentation gives us a number of pieces of information. First, the variable `target` is a measure of disease severity one year after diagnosis. So we will probably focus on predicting that variable. Also, we have descriptions of all of the variables, so we can label them in a way that is more helpful. Finally, there is a note stating that all the variables except `target` have been "mean centered and scaled to have squared length = 1 (x^2=1)". What that means is that the researchers transformed the data so that each of the 10 'predictor' variables would have similar values. Researchers do this to make it easier to compare effect sizes between variables. 

However, in this case the transformation they chose was not linear (remember the x^2), which makes it harder to transform the variables back later. Thankfully, we can read in the [original data](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html) from the source, and rename the columns so it is easier to remember what they mean

```{python}

diabetes_orig = pd.read_csv('https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt', sep='\t')

diabetes_orig = diabetes_orig.rename(columns=str.lower)

namedict = {'s1': 'totchol', 's2':'ldl', 's3':'hdl', 's4':'tch', 
's5':'ltg', 's6':'glucose', 'y':'target'}

diabetes_orig=diabetes_orig.rename(columns=namedict)

print(diabetes_orig.head())

print(diabetes_orig.describe())

```

That looks more like the data we were expecting!

If we still want our predictor variables to be scaled, we can use the linear  [Z-Score Transformation](https://www.listendata.com/2017/04/how-to-standardize-variable-in-regression.html) instead. The scikit-learn package has a class designed to do just that: `StandardScaler`.

```{python}

std_scaler = StandardScaler()
 
diabetes_scaled = std_scaler.fit_transform(diabetes_orig.to_numpy())
diabetes_scaled = pd.DataFrame(diabetes_scaled, columns=diabetes_orig.columns)

diabetes = pd.concat([diabetes_scaled.iloc[:,0:10], diabetes_orig.iloc[:,10:12]],
axis=1)

```

# Visualizing The Dataset 

Whenever we have a new dataset to analyze, a good second step is visualizing the variables in the dataset. For the Diabetes dataset, we are specifically interested in disease severity so we will be using `target`, a measure of diabetes disease progression.  Basically it tells us how bad a person's diabetes is. Higher numbers mean the diabetes is worse. Figure 1 shows every variable in the dataset plotted against our measure of disease progression in scatterplots. 

By looking at the scatterplots, we can get a good idea of which variables are likely to help us understand diabetes better, or at least better predict disease severity. Body Mass Index and Blood Triglycerides are positively correlated with more severe diabetes while High-Density Lipoproteins (Good Cholesteral) is possibly negatively correlated with severe diabetes. For the other variables, the data is too 'noisy' or spread out to tell from a visual inspection.
```{python}

#Names of X variables
x_names = ['Age', 'Sex', 'BMI', 'Blood Pressure', 'Total Cholestral', 
"LDL ('Bad Cholesteral')", "HDL ('Good Cholesteral')", 'Total/HDL', 
'Blood Triglycerides', 'Blood Sugar Level']

ncol = 2 #number of columns of subplots
nrow = int(len(x_names)/ncol) #number of rows of subplots

#create figure to display scatter plots of 
fig1, axs1 = plt.subplots(nrow, ncol, sharey=True, sharex=True,
figsize=(8,10))

for i, ax in enumerate(fig1.axes):
    ax.scatter(diabetes.iloc[:, i], diabetes.target, s=3)
    ax.set_xlabel(x_names[i])
    if i % 2 == 0:
        ax.set_ylabel('Disease Progression')

#Add Title
fig1.suptitle('Figure 1. Scatterplots of Diabetes Covariates')

#removing space between the plots horizontally 
plt.subplots_adjust(wspace=0, top=0.95, bottom=0.15)


plt.show()

```

# Quantifying the Dataset
While it is helpful to look at data visually, if we want to make statistical statements, we need to get down to the numbers.

# Relationships Between Variables: Linear Regression

The simplest relationship between two variables is a linear relationship:

$$y = m\cdot x + b$$

where $y$ is the variable we are interested in predicting, which is also called the dependent or response variable. $x$ is a variable we will use to predict $y$, and is either called the independent variable or the predictor variable. In our diabetes data, $y$ is our measure of disease progression (`target`) and $x$ could be any one of our other variables. For this example we will use BMI as our predictor variable. 

Linear regression is the process of finding a line (a model) that best fits our data. We call this line a model because it is a representation or summary of the relationship between the two variables. Most things in the world are not linearly related, but often times they are close enough that that a linear model can still be useful.

![](data/plots/allmodelsarewrong.jpeg){width=300}

Our linear model is summarized by two parameters: $m$ (slope) and $b$ (y-intercept). To find the parameters for the line that best fits our data, we minimize the squared differences between the $y$ values of our line and the $y$ values of our data. That is why this process is sometimes called "Least Squares Regression".

There are several steps estimating the parameters for a linear regression in python using the `scikit-learn` package. First we need to import the `LinearRegression` class, which we did at the top of the problem set. Next we need to create an instance of that class, which we will call `model`. For now we can use the default parameter settings. 

```{python}

model = LinearRegression()

```

Next we need to 'fit' the model. Fitting a model is providing that model with data that it can use to estimate the model parameters. In this case those parameters are $m$ and $b$. We use the `fit` method to fit a linear regression model, with the arguments being our predictor variable (BMI) and our response variable (target). This is like plugging in data into our linear regression formula. Because `scikit-learn` is based off of the package `numpy` we will need to use double brackets instead of single ones when referring to our DataFrame columns. 

```{python}

fit_model = model.fit(diabetes_orig[['bmi']], diabetes_orig[['target']])

```

Once we have fit the model, we can extract the parameters of our model using  
`coef_` for the slope ($m$) and `intercept_` for the y intercept ($b$). This will tell us how much we predict an increase in BMI will change disease severity.

```{python}

m = np.round(fit_model.coef_, 2)
b = np.round(fit_model.intercept_,2)

print(m)
print(b)
```

We can use these parameters to write out the equation of our linear model.

$$y = 10.23\cdot x - 117.77$$

This means that this model predicts an increase of 1 in BMI would lead to a predicted increase of approximately 10 in disease severity. To know the if that is a lot or a little, we would need to know more about how this measure of disease manifests in symptoms. We may also want to know how much an increasing someone's actual BMI from a healthy level (BMI=24.9) to being classified as obese (BMI = 30) effects their predicted disease severity. To do that we would just multiplty 5.1 by our slope.

```{python} 

m * (5.1)
```

Additionally this model predicts that someone with a BMI of zero would have a predicted disease severity of -117.77, this doesn't make sense biologically, but it is good to remember that our models have limits and if we use them to try to predict outcomes outside of the data range, we may get non-sensical results.

If we were plotting the line by hand, we would also use the parameters to plot the line of the model. However, in python, it is easier to use another method for plotting: the predicted y values.

In addition to using the `LinearRegression` class to fit models, we can also use it to predict values of the response variable (target) based on the values of the predictor variable (BMI). We can use the model to predict values of disease severity for the values of BMI of the participants in the study any given value of BMI:

```{python}

#predict disease severity for participants in the study
target_predicted = fit_model.predict(diabetes_orig[['bmi']])

```

Then we can plot those predicted values as a line, along with the scatterplot of our original data.

```{python}

fig2, ax2 = plt.subplots() #create figure

# Plot Linear model
ax2.plot(diabetes_orig['bmi'],target_predicted, color='black')
ax2.scatter(diabetes_orig['bmi'], diabetes_orig['target'], s=2)
#ax2.set_ylim([0,1]) # set y axis from 0-1 for full probability scale
ax2.set_title('Figure 2. Predicting Disease Severity Using BMI') # add title to figure
ax2.set_xlabel('BMI') # add x axis label
ax2.set_ylabel('Disease Severity') # add y axis label
plt.show() # show plot


```


# Questions

For this problem set please use the NBA Player dataset provided. This has per 36 Minute data from the 2021-2022 season and total points scored from the 2022-2023 season. The original source of the data is [BasketballReference.com](https://www.basketball-reference.com/)'s [21-22](https://www.basketball-reference.com/leagues/NBA_2022_per_minute.html) and [22-23](https://www.basketball-reference.com/leagues/NBA_2023_totals.html) season data. where you can find the documentation for the variables.

1. Describe each variable in the NBA Players dataset, including the units and data range if it is a numeric variable. 
2. Prediction vs. Inference
   a. What is a question of prediction we could ask about total points scored in the 22-23 season using the `NBA Player` data set.
   b. What is an inferential question we could ask about total points scored in the 22-23 season using the `NBA Player` data set. 
3. Visualization: create a figure with scatterplots showing how total points scored changes with several of the numeric predictor variables in the dataset. Create these plots with both scaled and unscaled predictor variables. What are the differences, if any, between the plots?
4. Why do we want to minimize the squared differences, instead of just the differences, to find the line of best fit?
5. What are the arguments for the `LinearRegression()` function and what do they do?
6. Create a linear regression model where you predict the total points scored (or fantasy basketball score) in 2023 based on one variable from data from the 2022 data.
   a. What are the estimated values for the parameters in your model?
   b. What is the interpretation of those parameter values? 
7. Create a plot that includes both the line representing your model, and the original data you used to fit the model.

